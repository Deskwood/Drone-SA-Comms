model_name_or_path: C:/models/Llama-3.1-8B-Instruct
stage: sft
do_train: true
do_eval: true

dataset_dir: lora
dataset: l4_train
eval_dataset: l4_val
template: llama3
cutoff_len: 2048

finetuning_type: lora
lora_rank: 32
lora_alpha: 64
lora_dropout: 0.05

learning_rate: 0.0002
num_train_epochs: 2
per_device_train_batch_size: 1
gradient_accumulation_steps: 16

bf16: true
gradient_checkpointing: true
seed: 1
logging_steps: 10
save_steps: 200
save_total_limit: 2
output_dir: lora/output_l4_lora
overwrite_output_dir: true
