{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd60ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 00:22:26] (+0.000s) Logger initialized.\n",
      "[2025-07-18 00:22:26] (+0.000s) Load Config: config.json\n",
      "Model downloaded to C:\\Users\\CKamm\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\gguf\\4b\\1\\Qwen3-4B-Q8_0.gguf\n",
      "8 x 8 board created.\n",
      "[2025-07-18 00:22:26] (+0.530s) Creating figures based on configuration.\n",
      "Placed white king at position [4, 0].\n",
      "Placed white queen at position [3, 0].\n",
      "Placed white rook at position [0, 0].\n",
      "Placed white bishop at position [2, 0].\n",
      "Placed white bishop at position [5, 0].\n",
      "Placed white knight at position [1, 0].\n",
      "Placed white knight at position [6, 0].\n",
      "Placed white pawn at position [0, 1].\n",
      "Placed white pawn at position [1, 1].\n",
      "Placed white pawn at position [2, 1].\n",
      "Placed white pawn at position [3, 1].\n",
      "Placed white pawn at position [4, 1].\n",
      "Placed white pawn at position [5, 1].\n",
      "Placed white pawn at position [6, 1].\n",
      "Placed white pawn at position [7, 1].\n",
      "Placed black king at position [4, 7].\n",
      "Placed black queen at position [3, 7].\n",
      "Placed black rook at position [0, 7].\n",
      "Placed black bishop at position [2, 7].\n",
      "Placed black bishop at position [5, 7].\n",
      "Placed black knight at position [1, 7].\n",
      "Placed black knight at position [6, 7].\n",
      "Placed black pawn at position [0, 6].\n",
      "Placed black pawn at position [1, 6].\n",
      "Placed black pawn at position [2, 6].\n",
      "Placed black pawn at position [3, 6].\n",
      "Placed black pawn at position [4, 6].\n",
      "Placed black pawn at position [5, 6].\n",
      "Placed black pawn at position [6, 6].\n",
      "Placed black pawn at position [7, 6].\n",
      "Figure white king at [4, 0] can target (5, 0).\n",
      "Figure white king at [4, 0] can target (3, 0).\n",
      "Figure white king at [4, 0] can target (4, 1).\n",
      "Figure white king at [4, 0] can target (5, 1).\n",
      "Figure white king at [4, 0] can target (3, 1).\n",
      "Figure white queen at [3, 0] can target (4, 0).\n",
      "Figure white queen at [3, 0] can target (2, 0).\n",
      "Figure white queen at [3, 0] can target (3, 1).\n",
      "Figure white queen at [3, 0] can target (4, 1).\n",
      "Figure white queen at [3, 0] can target (2, 1).\n",
      "Figure white rook at [0, 0] can target (1, 0).\n",
      "Figure white rook at [0, 0] can target (0, 1).\n",
      "Figure white bishop at [2, 0] can target (3, 1).\n",
      "Figure white bishop at [2, 0] can target (1, 1).\n",
      "Figure white bishop at [5, 0] can target (6, 1).\n",
      "Figure white bishop at [5, 0] can target (4, 1).\n",
      "Figure white knight at [1, 0] can target (3, 1).\n",
      "Figure white knight at [1, 0] can target (2, 2).\n",
      "Figure white knight at [1, 0] can target (0, 2).\n",
      "Figure white knight at [6, 0] can target (4, 1).\n",
      "Figure white knight at [6, 0] can target (7, 2).\n",
      "Figure white knight at [6, 0] can target (5, 2).\n",
      "Figure white pawn at [0, 1] can target (1, 2).\n",
      "Figure white pawn at [1, 1] can target (2, 2).\n",
      "Figure white pawn at [1, 1] can target (0, 2).\n",
      "Figure white pawn at [2, 1] can target (3, 2).\n",
      "Figure white pawn at [2, 1] can target (1, 2).\n",
      "Figure white pawn at [3, 1] can target (4, 2).\n",
      "Figure white pawn at [3, 1] can target (2, 2).\n",
      "Figure white pawn at [4, 1] can target (5, 2).\n",
      "Figure white pawn at [4, 1] can target (3, 2).\n",
      "Figure white pawn at [5, 1] can target (6, 2).\n",
      "Figure white pawn at [5, 1] can target (4, 2).\n",
      "Figure white pawn at [6, 1] can target (7, 2).\n",
      "Figure white pawn at [6, 1] can target (5, 2).\n",
      "Figure white pawn at [7, 1] can target (6, 2).\n",
      "Figure black king at [4, 7] can target (5, 7).\n",
      "Figure black king at [4, 7] can target (3, 7).\n",
      "Figure black king at [4, 7] can target (4, 6).\n",
      "Figure black king at [4, 7] can target (3, 6).\n",
      "Figure black king at [4, 7] can target (5, 6).\n",
      "Figure black queen at [3, 7] can target (4, 7).\n",
      "Figure black queen at [3, 7] can target (2, 7).\n",
      "Figure black queen at [3, 7] can target (3, 6).\n",
      "Figure black queen at [3, 7] can target (2, 6).\n",
      "Figure black queen at [3, 7] can target (4, 6).\n",
      "Figure black rook at [0, 7] can target (1, 7).\n",
      "Figure black rook at [0, 7] can target (0, 6).\n",
      "Figure black bishop at [2, 7] can target (1, 6).\n",
      "Figure black bishop at [2, 7] can target (3, 6).\n",
      "Figure black bishop at [5, 7] can target (4, 6).\n",
      "Figure black bishop at [5, 7] can target (6, 6).\n",
      "Figure black knight at [1, 7] can target (3, 6).\n",
      "Figure black knight at [1, 7] can target (2, 5).\n",
      "Figure black knight at [1, 7] can target (0, 5).\n",
      "Figure black knight at [6, 7] can target (4, 6).\n",
      "Figure black knight at [6, 7] can target (7, 5).\n",
      "Figure black knight at [6, 7] can target (5, 5).\n",
      "Figure black pawn at [0, 6] can target (1, 5).\n",
      "Figure black pawn at [1, 6] can target (2, 5).\n",
      "Figure black pawn at [1, 6] can target (0, 5).\n",
      "Figure black pawn at [2, 6] can target (3, 5).\n",
      "Figure black pawn at [2, 6] can target (1, 5).\n",
      "Figure black pawn at [3, 6] can target (4, 5).\n",
      "Figure black pawn at [3, 6] can target (2, 5).\n",
      "Figure black pawn at [4, 6] can target (5, 5).\n",
      "Figure black pawn at [4, 6] can target (3, 5).\n",
      "Figure black pawn at [5, 6] can target (6, 5).\n",
      "Figure black pawn at [5, 6] can target (4, 5).\n",
      "Figure black pawn at [6, 6] can target (7, 5).\n",
      "Figure black pawn at [6, 6] can target (5, 5).\n",
      "Figure black pawn at [7, 6] can target (6, 5).\n",
      "[2025-07-18 00:22:26] (+0.000s) Creating 1 drones.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup environment and imports\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "import json\n",
    "import pygame\n",
    "from llama_cpp import Llama\n",
    "import kagglehub\n",
    "\n",
    "COLORS = [\"white\", \"black\"]\n",
    "FIGURE_TYPES = [\"king\", \"queen\", \"rook\", \"bishop\", \"knight\", \"pawn\"]\n",
    "\n",
    "class TimestampedLogger:\n",
    "    def __init__(self, log_dir='logs', log_file='simulation.log'):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        self.log_path = os.path.join(log_dir, log_file)\n",
    "        logging.basicConfig(filename=self.log_path, level=logging.INFO, filemode='w')\n",
    "        self.start_time = time.time()\n",
    "        self.last_time = self.start_time\n",
    "        self.log(\"Logger initialized.\")\n",
    "\n",
    "    def _now(self):\n",
    "        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    def _duration(self):\n",
    "        current_time = time.time()\n",
    "        duration = current_time - self.last_time\n",
    "        self.last_time = current_time\n",
    "        return f\"{duration:.3f}s\"\n",
    "\n",
    "    def log(self, message):\n",
    "        timestamp = self._now()\n",
    "        duration = self._duration()\n",
    "        log_message = f\"[{timestamp}] (+{duration}) {message}\"\n",
    "        print(log_message)\n",
    "        logging.info(log_message)\n",
    "LOGGER = TimestampedLogger()\n",
    "\n",
    "def load_config(config_path: str = \"config.json\") -> dict:\n",
    "    \"\"\"Loads the configuration file.\"\"\"\n",
    "    LOGGER.log(f\"Load Config: {config_path}\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "CONFIG = load_config(\"config.json\")\n",
    "\n",
    "class _Figure:\n",
    "    \"\"\"Represents a figure on the board.\"\"\"\n",
    "    def __init__(self, position: Tuple[int, int], color: str, figure_type: str):\n",
    "        self.position = position\n",
    "        self.color = color\n",
    "        self.figure_type = figure_type\n",
    "\n",
    "        self.target_positions = []  # List of target positions this figure can attack or defend\n",
    "\n",
    "    def calculate_figure_targets(self, board: List[List['_Tile']]):\n",
    "        \"\"\"Generates a list of target positions based on the figure's movement rules.\"\"\"\n",
    "        # Calculate movement rules based on figure type\n",
    "        directions = []\n",
    "        max_board_size = max(CONFIG[\"board\"][\"width\"], CONFIG[\"board\"][\"height\"])\n",
    "        max_range = max_board_size if self.figure_type in [\"queen\", \"rook\", \"bishop\"] else 1\n",
    "        if self.figure_type == \"king\" or self.figure_type == \"queen\":\n",
    "            directions = [(1, 0), (-1, 0), (0, 1), (0, -1), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
    "        elif self.figure_type == \"rook\":\n",
    "            directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "        elif self.figure_type == \"bishop\":\n",
    "            directions = [(1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
    "        elif self.figure_type == \"knight\":\n",
    "            directions = [(2, 1), (2, -1), (-2, 1), (-2, -1), (1, 2), (1, -2), (-1, 2), (-1, -2)]\n",
    "        elif self.figure_type == \"pawn\":\n",
    "            # Pawns can only capture diagonally forward\n",
    "            if self.color == \"white\":\n",
    "                directions = [(1, 1), (-1, 1)]\n",
    "            else:\n",
    "                directions = [(1, -1), (-1, -1)]\n",
    "\n",
    "        # Generate valid target positions based on its movement rules\n",
    "        for direction in directions:\n",
    "            for dr in range(1, max_range + 1):\n",
    "                dx = direction[0] * dr\n",
    "                dy = direction[1] * dr\n",
    "                target_position = (self.position[0] + dx, self.position[1] + dy)\n",
    "                if      0 <= target_position[0] < CONFIG[\"board\"][\"width\"] \\\n",
    "                    and 0 <= target_position[1] < CONFIG[\"board\"][\"height\"]:\n",
    "                    self.target_positions.append(target_position)\n",
    "                    print(f\"Figure {self.color} {self.figure_type} at {self.position} can target {target_position}.\")\n",
    "                    if board[target_position[0]][target_position[1]].figure is not None:\n",
    "                        # Stop if we hit another figure\n",
    "                        break\n",
    "\n",
    "class _Drone:\n",
    "    \"\"\"Represents a drone in the simulation.\"\"\"\n",
    "    def __init__(self, id: int, position: Tuple[int, int], model_path: str, rules: str, color: str = \"white\"):\n",
    "        self.id = id\n",
    "        self.position = position\n",
    "        self.color = color\n",
    "        self.model_path = model_path\n",
    "        self.memory = [{\"rules\": rules}]  # Memory of past actions or observations\n",
    "\n",
    "    def generate_model_response(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1280) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from a Qwen GGUF model using llama-cpp.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to the language model.\n",
    "            temperature (float): Sampling temperature.\n",
    "            max_tokens (int): Maximum tokens to generate in the response.\n",
    "\n",
    "        Returns:\n",
    "            str: The model's text response.\n",
    "        \"\"\"\n",
    "        llm = Llama(\n",
    "            model_path=self.model_path,\n",
    "            n_ctx=2048,\n",
    "            temperature=temperature,\n",
    "            n_threads=os.cpu_count() // 2  # Adjust to your CPU\n",
    "        )\n",
    "\n",
    "        response = llm(prompt, max_tokens=max_tokens, stop=[\"</s>\"], echo=False)\n",
    "        # return response[\"choices\"][0][\"text\"].strip()\n",
    "        return str(response)\n",
    "\n",
    "    def get_action(self):\n",
    "        \"\"\"Determines the action for the drone based on its rules.\"\"\"\n",
    "        # Placeholder for action logic: move, wait, or broadcast\n",
    "        LOGGER.log(f\"Drone {self.id} at position {self.position} is taking action.\")\n",
    "        model_input = self.memory[-1]  # Use the last memory entry as input\n",
    "        print(self.generate_model_response(f\"Drone {self.id} at position {self.position} with memory {model_input} decides its next action.\"))\n",
    "\n",
    "class _Tile:\n",
    "    \"\"\"Represents a tile on the board.\"\"\"\n",
    "    def __init__(self, x: int, y: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.targeted_by = {\"white\": 0, \"black\": 0}\n",
    "        self.figure = None\n",
    "        self.drones = []\n",
    "\n",
    "    def set_figure(self, figure: _Figure):\n",
    "        \"\"\"Sets a piece on the tile.\"\"\"\n",
    "        self.figure = figure\n",
    "\n",
    "    def add_drone(self, drone: _Drone):\n",
    "        \"\"\"Adds a drone to the tile.\"\"\"\n",
    "        assert drone not in self.drones\n",
    "        self.drones.append(drone)\n",
    "\n",
    "    def remove_drone(self, drone: _Drone):\n",
    "        \"\"\"Removes a drone from the tile.\"\"\"\n",
    "        if drone in self.drones:\n",
    "            self.drones.remove(drone)\n",
    "\n",
    "    def reset_targeted_by_amounts(self):\n",
    "        \"\"\"Resets the threat and defend levels.\"\"\"\n",
    "        self.targeted_by = {\"white\": 0, \"black\": 0}\n",
    "\n",
    "    def add_targeted_by_amount(self, color: str, amount: int = 1):\n",
    "        \"\"\"Adds to the targeted by amount for a color.\"\"\"\n",
    "        self.targeted_by[color] += amount\n",
    "\n",
    "class _SimulationGUI:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the GUI for the simulation.\"\"\"\n",
    "        self.grid_size = SIM.grid_size\n",
    "        \n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        gui_config = CONFIG[\"gui\"]\n",
    "        self.screen = pygame.display.set_mode(\n",
    "            (self.grid_size[0] * (gui_config[\"cell_size\"] + gui_config[\"margin\"]) + gui_config[\"margin\"],\n",
    "             self.grid_size[1] * (gui_config[\"cell_size\"] + gui_config[\"margin\"]) + gui_config[\"margin\"])\n",
    "        )\n",
    "        pygame.display.set_caption(f\"Simulation - Round {1}.1/{SIM.max_rounds}.{SIM.num_drones}\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "    def draw_field(self):\n",
    "        \"\"\"Draws the board, king, and drones using the current state.\"\"\"\n",
    "        gui_config = CONFIG[\"gui\"]\n",
    "        cell_size = gui_config[\"cell_size\"]\n",
    "        margin = gui_config[\"margin\"]\n",
    "        grid_width, grid_height = self.grid_size\n",
    "\n",
    "        self.screen.fill(gui_config[\"background_color\"])\n",
    "        font = pygame.font.SysFont(None, 16)\n",
    "\n",
    "        for x in range(grid_width):\n",
    "            for y in range(grid_height):\n",
    "                rect = pygame.Rect(\n",
    "                    x * (cell_size + margin) + margin,\n",
    "                    y * (cell_size + margin) + margin,\n",
    "                    cell_size,\n",
    "                    cell_size\n",
    "                )\n",
    "                pygame.draw.rect(self.screen, gui_config[\"grid_color\"], rect)\n",
    "\n",
    "                tile = SIM.board[x][y]\n",
    "\n",
    "                # Draw King\n",
    "                if tile.figure and tile.figure.figure_type == \"king\":\n",
    "                    pygame.draw.circle(self.screen, gui_config[\"king_color\"], rect.center, cell_size // 3)\n",
    "\n",
    "                # Draw drones (multiple per tile supported)\n",
    "                total = len(tile.drones)\n",
    "                if total > 0:\n",
    "                    angle_step = 360 / total if total > 1 else 0\n",
    "                    radius = cell_size // 6\n",
    "\n",
    "                    for idx in range(len(tile.drones)):\n",
    "                        angle = angle_step * idx\n",
    "                        offset = pygame.math.Vector2(0, 0)\n",
    "                        if total > 1:\n",
    "                            offset = pygame.math.Vector2(1, 0).rotate(angle) * (cell_size // 4)\n",
    "                        center = (rect.centerx + int(offset.x), rect.centery + int(offset.y))\n",
    "\n",
    "                        pygame.draw.circle(self.screen, gui_config[\"drone_color\"], center, radius)\n",
    "                        text = font.render(str(idx + 1), True, gui_config[\"text_color\"])\n",
    "                        text_rect = text.get_rect(center=center)\n",
    "                        self.screen.blit(text, text_rect)\n",
    "                        \n",
    "        pygame.display.flip()\n",
    "\n",
    "class Simulation:\n",
    "    \"\"\"Tracks the state of the simulation.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize game state\n",
    "        self.turn = 1 # Which drone's turn it is\n",
    "        self.round = 1 # Which round of the game it is\n",
    "        self.rules = \"\"        \n",
    "        with open(CONFIG[\"simulation\"][\"rules_path\"], \"r\") as f:\n",
    "            self.rules = f.read().replace(\"NUMBER_OF_DRONES\", str(CONFIG[\"simulation\"][\"num_drones\"]))\n",
    "        self.grid_size = (CONFIG[\"board\"][\"width\"], CONFIG[\"board\"][\"height\"])\n",
    "        self.max_rounds = CONFIG[\"simulation\"][\"max_rounds\"]\n",
    "        self.num_drones = CONFIG[\"simulation\"][\"num_drones\"]\n",
    "        self.model_path = os.path.join(kagglehub.model_download(CONFIG[\"simulation\"][\"model\"]), CONFIG[\"simulation\"][\"model_name\"])\n",
    "        print(f\"Model downloaded to {self.model_path}\")\n",
    "\n",
    "        # Create the board\n",
    "        self.board = [[_Tile(x, y) for y in range(self.grid_size[1])] for x in range(self.grid_size[0])]\n",
    "        print(len(self.board), \"x\", len(self.board[0]), \"board created.\")\n",
    "        self.figures = []  # List of all figures on the board\n",
    "        self.drones = []  # List of drones in the simulation\n",
    "\n",
    "        # Create, place and calculate figures\n",
    "        self._create_figures()\n",
    "\n",
    "        # Create and place drones\n",
    "        self.drone_base = self.figures[0].position # Assuming the first figure is the drone base (white king)\n",
    "        self._create_drones()\n",
    "\n",
    "        # Initialize the GUI\n",
    "        self.gui = _SimulationGUI()\n",
    "\n",
    "    def _create_figures(self):\n",
    "        \"\"\"Generates and places all figures based on CONFIG[\"figures\"].\"\"\"\n",
    "        # Create figures based on the configuration\n",
    "        LOGGER.log(\"Creating figures based on configuration.\")\n",
    "        self.figures = []\n",
    "        for color in COLORS:\n",
    "            for figure_type in FIGURE_TYPES:\n",
    "                for position in CONFIG[\"figures\"][color][figure_type]:\n",
    "                    self.figures.append(_Figure(position, color, figure_type))\n",
    "\n",
    "        # Place figures on the board\n",
    "        for figure in self.figures:\n",
    "            self.board[figure.position[0]][figure.position[1]].set_figure(figure)\n",
    "            print(f\"Placed {figure.color} {figure.figure_type} at position {figure.position}.\")\n",
    "        \n",
    "        # Calculate targets for each figure\n",
    "        for figure in self.figures:\n",
    "            figure.calculate_figure_targets(self.board)\n",
    "\n",
    "        # Add targeted by amounts to the tiles based on figure targets\n",
    "        for figure in self.figures:\n",
    "            for target_position in figure.target_positions:\n",
    "                target_tile = self.board[target_position[0]][target_position[1]]\n",
    "                target_tile.add_targeted_by_amount(figure.color, 1)\n",
    "\n",
    "    def _create_drones(self):\n",
    "        \"\"\"Generate drones.\"\"\"\n",
    "        LOGGER.log(f\"Creating {self.num_drones} drones.\")\n",
    "        for i in range(self.num_drones):\n",
    "            drone = _Drone(id=i+1, position=self.drone_base, model_path=self.model_path, rules=self.rules)\n",
    "            self.drones.append(drone)\n",
    "            \n",
    "        # Place drones on the board\n",
    "        for drone in self.drones:\n",
    "            tile = self.board[drone.position[0]][drone.position[1]]\n",
    "            tile.add_drone(drone)\n",
    "\n",
    "SIM = Simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d16912d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define real-time simulation GUI using Pygame\n",
    "def run_simulation(sim: Simulation):\n",
    "    \"\"\"Runs the simulation with real-time GUI display using Pygame.\"\"\"\n",
    "\n",
    "    max_rounds = CONFIG[\"simulation\"].get(\"max_rounds\", 10)\n",
    "    delay = CONFIG[\"simulation\"].get(\"delay\", 300)\n",
    "\n",
    "    directions = [(-1, -1), (-1, 0), (-1, 1),\n",
    "                  ( 0, -1), ( 0, 0), ( 0, 1),\n",
    "                  ( 1, -1), ( 1, 0), ( 1, 1)]\n",
    "\n",
    "    running = True\n",
    "    pygame.display.set_caption(f\"Simulation - Round {1}.1/{max_rounds}.{SIM.num_drones}\")\n",
    "\n",
    "    for sim_round in range(max_rounds):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                break\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        SIM.round = sim_round + 1\n",
    "\n",
    "        # Move each drone\n",
    "        for drone in sim.drones:\n",
    "            SIM.turn = drone.id\n",
    "            pygame.display.set_caption(f\"Simulation - Round {sim_round + 1}.{drone.id}/{max_rounds}.{SIM.num_drones}\")\n",
    "            \n",
    "            x, y = drone.position\n",
    "            old_tile = sim.board[x][y]\n",
    "\n",
    "            # Determine figures on the current tile\n",
    "            if old_tile.figure:\n",
    "                LOGGER.log(f\"Drone {drone.id} is on tile ({x}, {y}) with figure: {old_tile.figure.color} {old_tile.figure.figure_type}\")\n",
    "\n",
    "            # Determine drones on the current tile\n",
    "            if old_tile.drones:\n",
    "                LOGGER.log(f\"Drone {drone.id} is on tile ({x}, {y}) with drones: {', '.join(str(drone.id) for drone in old_tile.drones)}\")\n",
    "\n",
    "            drone.get_action()  # Get action from the drone's model\n",
    "            \n",
    "\n",
    "            # Remove drone from current tile before movement\n",
    "            if drone in old_tile.drones:\n",
    "                old_tile.drones.remove(drone)\n",
    "\n",
    "            # Determine valid moves\n",
    "            valid_moves = []\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if 0 <= nx < SIM.grid_size[0] and 0 <= ny < SIM.grid_size[1]:\n",
    "                    valid_moves.append((nx, ny))\n",
    "\n",
    "            # Move drone\n",
    "            if valid_moves:\n",
    "                new_pos = random.choice(valid_moves)\n",
    "                drone.position = new_pos\n",
    "                new_tile = SIM.board[new_pos[0]][new_pos[1]]\n",
    "                new_tile.drones.append(drone)\n",
    "            else:\n",
    "                # If no valid moves, re-add to current tile\n",
    "                old_tile.drones.append(drone)\n",
    "            \n",
    "            SIM.gui.draw_field()\n",
    "            pygame.time.delay(delay)\n",
    "        \n",
    "            LOGGER.log(f\"Round {SIM.round}.{SIM.turn} completed.\")\n",
    "\n",
    "\n",
    "\n",
    "    LOGGER.log(\"Simulation ended.\")\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42ed72eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 28 key-value pairs and 398 tensors from C:\\Users\\CKamm\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\gguf\\4b\\1\\Qwen3-4B-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 4B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen3\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 4B\n",
      "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n",
      "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n",
      "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 9728\n",
      "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  145 tensors\n",
      "llama_model_loader: - type q8_0:  253 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.98 GiB (8.50 BPW) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-18 00:22:27] (+1.132s) Launching real-time simulation with GUI...\n",
      "[2025-07-18 00:22:27] (+0.002s) Drone 1 is on tile (4, 0) with figure: white king\n",
      "[2025-07-18 00:22:27] (+0.000s) Drone 1 is on tile (4, 0) with drones: 1\n",
      "[2025-07-18 00:22:27] (+0.000s) Drone 1 at position [4, 0] is taking action.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 40960\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 9728\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 40960\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 4B\n",
      "print_info: model params     = 4.02 B\n",
      "print_info: general.name     = Qwen3 4B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 398 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4076.43 MiB\n",
      "............................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   288.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   301.75 MiB\n",
      "llama_context: graph nodes  = 1374\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen3 4B Instruct', 'general.architecture': 'qwen3', 'qwen3.context_length': '40960', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'general.basename': 'Qwen3', 'general.finetune': 'Instruct', 'general.size_label': '4B', 'tokenizer.ggml.pre': 'qwen2', 'qwen3.block_count': '36', 'qwen3.embedding_length': '2560', 'qwen3.feed_forward_length': '9728', 'qwen3.attention.head_count': '32', 'qwen3.attention.head_count_kv': '8', 'qwen3.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'false', 'qwen3.attention.key_length': '128', 'qwen3.attention.value_length': '128', 'tokenizer.ggml.model': 'gpt2', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.padding_token_id': '151643', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for index in range(ns.last_query_index, -1, -1) %}\\n    {%- set message = messages[index] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(\\'<tool_response>\\' in message.content and \\'</tool_response>\\' in message.content) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = message.content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = message.content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for index in range(ns.last_query_index, -1, -1) %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not('<tool_response>' in message.content and '</tool_response>' in message.content) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "llama_perf_context_print:        load time =   30133.89 ms\n",
      "llama_perf_context_print: prompt eval time =   30133.58 ms /   430 tokens (   70.08 ms per token,    14.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15455.75 ms /   127 runs   (  121.70 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   45854.57 ms /   557 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-036bd543-c59c-4d16-9536-8f47ecf89fc5', 'object': 'text_completion', 'created': 1752790948, 'model': 'C:\\\\Users\\\\CKamm\\\\.cache\\\\kagglehub\\\\models\\\\qwen-lm\\\\qwen-3\\\\gguf\\\\4b\\\\1\\\\Qwen3-4B-Q8_0.gguf', 'choices': [{'text': \" Drone 1 is at [4, 0] on a chessboard. The chessboard is 8x8. The drone can move to neighboring squares, which are adjacent horizontally, vertically, or diagonally. So, the possible moves from [4,0] are:\\n\\n[3,0], [3,1], [4,1], [5,0], [5,1], [4,-1] (but [4,-1] is invalid as it's off the board).\\n\\nSo the possible moves are:\\n\\n[3,0], [3,1], [4,1], [5,0\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 430, 'completion_tokens': 128, 'total_tokens': 558}}\n",
      "[2025-07-18 00:23:15] (+47.634s) Round 1.1 completed.\n",
      "[2025-07-18 00:23:15] (+0.000s) Simulation ended.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Optional launcher for real-time simulation with GUI in notebooks\n",
    "\n",
    "RUN_SIMULATION = True  # Set to False to disable auto-execution in notebooks\n",
    "\n",
    "if __name__ == \"__main__\" or RUN_SIMULATION:\n",
    "    LOGGER.log(\"Launching real-time simulation with GUI...\")\n",
    "    run_simulation(SIM)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
