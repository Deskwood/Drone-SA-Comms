{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "%pip install numpy pandas pygame matplotlib kagglehub torch torchvision torchaudio transformers sentencepiece accelerate bitsandbytes ollama ipykernel jupyter notebook pyperclip openai llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6807be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a model by index when calling run_model().\n",
      "[0] qwen3:30b\n",
      "[1] deepseek-r1:14b\n",
      "[2] llama3.1:8b\n",
      "\n",
      "Using model [1] deepseek-r1:14b\n",
      "{'model': 'deepseek-r1:14b', 'created_at': '2025-09-03T08:11:18.4731303Z', 'message': {'role': 'assistant', 'content': '<think>\\nOkay, so I need to figure out how to measure the performance of a machine learning model. Hmm, where do I start? I remember from my studies that there are different types of metrics depending on whether it\\'s classification or regression.\\n\\nLet me think about classification first. There\\'s accuracy, which is just the percentage of correct predictions. But wait, isn\\'t accuracy not always the best measure? Like, in cases where the classes are imbalanced, accuracy can be misleading. So maybe precision and recall are better. Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall is the ratio of correctly predicted positive observations to the actual positives. Oh right, and then there\\'s F1 score which combines both precision and recall.\\n\\nFor multiclass classification, accuracy might still be useful, but maybe micro and macro averages are important too. Macro average treats all classes equally, while micro looks at the overall performance across all classes. Then there\\'s confusion matrix, which gives a detailed breakdown of predictions versus actuals. ROC-AUC is another metric for binary classification, showing how well the model can distinguish between classes.\\n\\nNow, moving on to regression. For these models, metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are commonly used. MAE gives an average of the absolute differences, which is easy to interpret. MSE squares the errors, giving more weight to larger mistakes, but it\\'s also sensitive to outliers. RMSE takes the square root of MSE, making it interpretable in the same units as the target variable.\\n\\nThen there are evaluation metrics for clustering. Silhouette score measures how similar a sample is to its own cluster compared to others, with higher values indicating better clusters. The Davies-Bouldin index does the opposite; lower values mean better clustering.\\n\\nWhen it comes to model selection and tuning, cross-validation scores like cross-validated accuracy or RMSE are useful because they provide an estimate of how well the model generalizes by averaging results from multiple train-test splits. Learning curves can show if a model is overfitting or underfitting by plotting training and validation scores.\\n\\nFor deep learning models, metrics might include loss values during training, which indicate how well the model minimizes its error function. AUC-ROC for binary classification in deep learning is similar to traditional machine learning.BLEU score is used for evaluating text generation tasks, like translation or summarization.\\n\\nI also remember that it\\'s important to choose the right metric based on the problem. For example, if false positives are more problematic than false negatives, precision might be prioritized over recall. Or in a highly imbalanced dataset, F1 score would give a better picture than accuracy.\\n\\nWait, I should make sure I\\'m not missing any other metrics. Oh right, there\\'s also things like the confusion matrix for classification, which is more of a table but gives detailed insights. For regression, R-squared (coefficient of determination) is another metric that explains how much variance in the target variable is explained by the model.\\n\\nI think I\\'ve covered most common metrics across different types of machine learning tasks. Now, to structure this into a Python dictionary as the user requested.\\n</think>\\n\\nHere\\'s a structured summary of various performance metrics for machine learning models organized into a Python dictionary:\\n\\n```python\\n{\\n    \"Classification\": {\\n        \"Accuracy\": \"Percentage of correct predictions.\",\\n        \"Precision\": \"Ratio of correctly predicted positives to total predicted positives.\",\\n        \"Recall\": \"Ratio of correctly predicted positives to actual positives.\",\\n        \"F1 Score\": \"Combination of precision and recall (2 * (precision * recall) / (precision + recall)).\",\\n        \"Confusion Matrix\": \"Breakdown of predictions vs. actuals, useful for multiclass problems.\",\\n        \"ROC-AUC\": \"Performance metric for binary classification, measures the area under the ROC curve.\"\\n    },\\n    \"Regression\": {\\n        \"Mean Absolute Error (MAE)\": \"Average of absolute differences between predictions and actual values.\",\\n        \"Mean Squared Error (MSE)\": \"Average of squared differences, emphasizes larger errors due to squaring.\",\\n        \"Root Mean Squared Error (RMSE)\": \"Square root of MSE, providing interpretable units.\",\\n        \"R-squared\": \"Explains variance in target variable explained by the model.\"\\n    },\\n    \"Clustering\": {\\n        \"Silhouette Score\": \"Measures similarity of samples to their own cluster vs. others, higher is better.\",\\n        \"Davies-Bouldin Index\": \"Lower values indicate better clustering.\"\\n    },\\n    \"Model Selection & Tuning\": {\\n        \"Cross-Validation Scores\": \"Estimate generalization performance by averaging across multiple splits.\",\\n        \"Learning Curves\": \"Plot training and validation scores to detect overfitting or underfitting.\"\\n    },\\n    \"Deep Learning\": {\\n        \"Loss Value\": \"Minimizes error function during training, indicating model performance.\",\\n        \"AUC-ROC\": \"Binary classification metric similar to traditional methods.\",\\n        \"BLEU Score\": \"Evaluates text generation tasks like translation or summarization.\"\\n    },\\n    \"Considerations\": {\\n        \"Metric Choice\": \"Select based on problem context; e.g., prioritize precision over recall if false positives are costly.\"\\n    }\\n}\\n```\\n\\nThis dictionary organizes metrics by their application areas, providing a clear and concise reference for evaluating different types of machine learning models.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 11394934900, 'load_duration': 1566059500, 'prompt_eval_count': 23, 'prompt_eval_duration': 82536000, 'eval_count': 1113, 'eval_duration': 9745135800}\n",
      "\n",
      "Model reply:\n",
      " <think>\n",
      "Okay, so I need to figure out how to measure the performance of a machine learning model. Hmm, where do I start? I remember from my studies that there are different types of metrics depending on whether it's classification or regression.\n",
      "\n",
      "Let me think about classification first. There's accuracy, which is just the percentage of correct predictions. But wait, isn't accuracy not always the best measure? Like, in cases where the classes are imbalanced, accuracy can be misleading. So maybe precision and recall are better. Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall is the ratio of correctly predicted positive observations to the actual positives. Oh right, and then there's F1 score which combines both precision and recall.\n",
      "\n",
      "For multiclass classification, accuracy might still be useful, but maybe micro and macro averages are important too. Macro average treats all classes equally, while micro looks at the overall performance across all classes. Then there's confusion matrix, which gives a detailed breakdown of predictions versus actuals. ROC-AUC is another metric for binary classification, showing how well the model can distinguish between classes.\n",
      "\n",
      "Now, moving on to regression. For these models, metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are commonly used. MAE gives an average of the absolute differences, which is easy to interpret. MSE squares the errors, giving more weight to larger mistakes, but it's also sensitive to outliers. RMSE takes the square root of MSE, making it interpretable in the same units as the target variable.\n",
      "\n",
      "Then there are evaluation metrics for clustering. Silhouette score measures how similar a sample is to its own cluster compared to others, with higher values indicating better clusters. The Davies-Bouldin index does the opposite; lower values mean better clustering.\n",
      "\n",
      "When it comes to model selection and tuning, cross-validation scores like cross-validated accuracy or RMSE are useful because they provide an estimate of how well the model generalizes by averaging results from multiple train-test splits. Learning curves can show if a model is overfitting or underfitting by plotting training and validation scores.\n",
      "\n",
      "For deep learning models, metrics might include loss values during training, which indicate how well the model minimizes its error function. AUC-ROC for binary classification in deep learning is similar to traditional machine learning.BLEU score is used for evaluating text generation tasks, like translation or summarization.\n",
      "\n",
      "I also remember that it's important to choose the right metric based on the problem. For example, if false positives are more problematic than false negatives, precision might be prioritized over recall. Or in a highly imbalanced dataset, F1 score would give a better picture than accuracy.\n",
      "\n",
      "Wait, I should make sure I'm not missing any other metrics. Oh right, there's also things like the confusion matrix for classification, which is more of a table but gives detailed insights. For regression, R-squared (coefficient of determination) is another metric that explains how much variance in the target variable is explained by the model.\n",
      "\n",
      "I think I've covered most common metrics across different types of machine learning tasks. Now, to structure this into a Python dictionary as the user requested.\n",
      "</think>\n",
      "\n",
      "Here's a structured summary of various performance metrics for machine learning models organized into a Python dictionary:\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"Classification\": {\n",
      "        \"Accuracy\": \"Percentage of correct predictions.\",\n",
      "        \"Precision\": \"Ratio of correctly predicted positives to total predicted positives.\",\n",
      "        \"Recall\": \"Ratio of correctly predicted positives to actual positives.\",\n",
      "        \"F1 Score\": \"Combination of precision and recall (2 * (precision * recall) / (precision + recall)).\",\n",
      "        \"Confusion Matrix\": \"Breakdown of predictions vs. actuals, useful for multiclass problems.\",\n",
      "        \"ROC-AUC\": \"Performance metric for binary classification, measures the area under the ROC curve.\"\n",
      "    },\n",
      "    \"Regression\": {\n",
      "        \"Mean Absolute Error (MAE)\": \"Average of absolute differences between predictions and actual values.\",\n",
      "        \"Mean Squared Error (MSE)\": \"Average of squared differences, emphasizes larger errors due to squaring.\",\n",
      "        \"Root Mean Squared Error (RMSE)\": \"Square root of MSE, providing interpretable units.\",\n",
      "        \"R-squared\": \"Explains variance in target variable explained by the model.\"\n",
      "    },\n",
      "    \"Clustering\": {\n",
      "        \"Silhouette Score\": \"Measures similarity of samples to their own cluster vs. others, higher is better.\",\n",
      "        \"Davies-Bouldin Index\": \"Lower values indicate better clustering.\"\n",
      "    },\n",
      "    \"Model Selection & Tuning\": {\n",
      "        \"Cross-Validation Scores\": \"Estimate generalization performance by averaging across multiple splits.\",\n",
      "        \"Learning Curves\": \"Plot training and validation scores to detect overfitting or underfitting.\"\n",
      "    },\n",
      "    \"Deep Learning\": {\n",
      "        \"Loss Value\": \"Minimizes error function during training, indicating model performance.\",\n",
      "        \"AUC-ROC\": \"Binary classification metric similar to traditional methods.\",\n",
      "        \"BLEU Score\": \"Evaluates text generation tasks like translation or summarization.\"\n",
      "    },\n",
      "    \"Considerations\": {\n",
      "        \"Metric Choice\": \"Select based on problem context; e.g., prioritize precision over recall if false positives are costly.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "This dictionary organizes metrics by their application areas, providing a clear and concise reference for evaluating different types of machine learning models.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "# Default performance settings tuned for ~32 GB VRAM\n",
    "MODEL_DEFAULTS = {\n",
    "    \"qwen3:30b\":       {\"num_ctx\": 16384, \"num_batch\": 4096, \"num_predict\": 2048},\n",
    "    \"deepseek-r1:14b\": {\"num_ctx\": 16384, \"num_batch\": 6144, \"num_predict\": 2048},\n",
    "    \"llama3.1:8b\":     {\"num_ctx\": 16384, \"num_batch\": 6144, \"num_predict\": 2048},\n",
    "}\n",
    "\n",
    "def list_models() -> List[str]:\n",
    "    \"\"\"Return all locally installed Ollama models (names only).\"\"\"\n",
    "    url = f\"{OLLAMA_HOST}/api/tags\"\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    models = [m[\"name\"] for m in r.json().get(\"models\", [])]\n",
    "    for i, name in enumerate(models):\n",
    "        print(f\"[{i}] {name}\")\n",
    "    return models\n",
    "\n",
    "def run_model(\n",
    "    prompt: str,\n",
    "    model_index: int = 0,\n",
    "    messages: Optional[List[Dict[str, str]]] = None,\n",
    "    options: Optional[Dict] = None,\n",
    "    timeout: int = 600,\n",
    ") -> str:\n",
    "    \"\"\"Run a chat prompt on a selected Ollama model by index.\"\"\"\n",
    "\n",
    "    models = list_models()\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No models installed in Ollama.\")\n",
    "\n",
    "    if model_index < 0 or model_index >= len(models):\n",
    "        raise IndexError(f\"Invalid model index {model_index}, available 0..{len(models)-1}\")\n",
    "\n",
    "    model = models[model_index]\n",
    "    print(f\"\\nUsing model [{model_index}] {model}\")\n",
    "\n",
    "    # Build default messages\n",
    "    if messages is None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"I respond in form of a python dictionary.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "    # Merge defaults\n",
    "    base_opts = {\n",
    "        \"num_gpu\": 999,\n",
    "        \"num_thread\": 16,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    opts = {**base_opts, **MODEL_DEFAULTS.get(model, {}), **(options or {})}\n",
    "\n",
    "    payload = {\"model\": model, \"messages\": messages, \"options\": opts, \"stream\": False}\n",
    "    url = f\"{OLLAMA_HOST}/api/chat\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    r = requests.post(url, json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    dt = time.time() - t0\n",
    "    data = r.json()\n",
    "    text = data[\"message\"][\"content\"]\n",
    "    # print(f\"[{model}] Elapsed {dt:.2f}s, {len(text)} chars\")\n",
    "    print(data)\n",
    "    return text\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Show available models\n",
    "    print(\"\\nSelect a model by index when calling run_model().\")\n",
    "\n",
    "    # Example: run on index 1 (deepseek-r1:14b)\n",
    "    prompt = \"Tell me a way to measure your model's performance.\"\n",
    "    reply = run_model(prompt, model_index=1)\n",
    "    print(\"\\nModel reply:\\n\", reply)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
